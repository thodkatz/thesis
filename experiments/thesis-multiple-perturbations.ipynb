{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thesis architecture leveraging multiple perturbations via multi-task learning\n",
    "\n",
    "We will attempt to use an VAE scheme for each perturbation type. Similarly, with scButterfly, and UnitedNet, the encoder\n",
    "and the decoder of the control RNA will be shared across the perturbations. We hope that sharing it, we will help each task individually.\n",
    "\n",
    "We will use the VAE implementation of scbutterfly, but to make our experiments easier, we could ditch some part of the architecture, and then we can enrich it afterwards. Based on the ablation study of scPreGan for example, they showed that each part of their architecture, including the discriminators are important.\n",
    "\n",
    "For our ablation study if multi-task learning is beneficial, we could focus on simple architectures with a few losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concerns\n",
    "\n",
    "- Encoding each perturbation seperately increase the computational cost. For datasets with >100 perturbation types, how this can be handled? This would be an open ended question, and the limitation of our architecture. The single encoder architectures such as CPA, and scGEN, don't have this limitation, but based on my understanding wouldn't be possible to be leveraged for multi-task learning. These work mostly as representation learning architectures, where the tasks are solved with post-processing, downstream ways."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
